---
title: "R_Churn_dataset_Kaggle"
author: "Bferrol"
date: "18/6/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Importación de datos y primeras observaciones

#### Funciones auxiliares para cargar paquetes y lista de paquetes

```{r}
prepare_packages <- function(packages){
  # Chequeamos que paquetes no estan instalados:
  non_intalled <- packages[!(packages %in% installed.packages()[, "Package"])]
  # En caso de existir alguno aún no instalado, lo instalamos:
  if (length(non_intalled)) 
    install.packages(non_intalled, dependencies = TRUE)
  # Cargamos toda la lista de paquetes:
  sapply(packages, require, character.only = TRUE)
}



packages <- c("tidyverse",
              "MASS",
              "car",
              "binr",
              "e1071",
              "caret",
              "cowplot",
              "caTools",
              "pROC",
              "ggcorrplot",
              "data.table",
              "Information",
              "rpart",
              "rpart.plot",
              "xgboost",
              "ROCR",
              "pROC",
              "h2o",
              "dummies",
              "fastAdaboost"
)

prepare_packages(packages)
# Installation of the doSNOW parallel library with all dependencies
doInstall <- TRUE # Change to FALSE if you don't want packages installed.
toInstall <- c("doSNOW") 
if((doInstall) && (!is.element(toInstall, installed.packages()[,1])))
{
    cat("Please install required package. Select server:"); chooseCRANmirror();
    install.packages(toInstall, dependencies = c("Depends", "Imports")) 
}
# load doSnow and (parallel for CPU info) library
library(doSNOW)
library(parallel)
# For doSNOW one can increase up to 128 nodes
# Each node requires 44 Mbyte RAM under WINDOWS.
# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
cat(nCores, " cores detected.")
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
cat(nThreads, " threads detected.")
# Create doSNOW compute cluster (try 64)
# One can increase up to 128 nodes
# Each node requires 44 Mbyte RAM under WINDOWS.
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
#get info
getDoParWorkers(); getDoParName();
# insert parallel computation here
        
# stop cluster and remove clients
stopCluster(cluster); print("Cluster stopped.")
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
```


Importamos el csv que fue previamente guardado en un `Azure blob storage`:
```{r}
dataset <- read.csv("https://bferrolfilesstorage.blob.core.windows.net/icemd/WA_Fn-UseC_-Telco-Customer-Churn.csv")
```

Veamos qué contiene el dataset:
```{r}
glimpse(dataset)

```

Están todos los factor booleanos en strings pero, el SeniorCitizen no. Cambiémoslo a YES/NO:
```{r}
dataset$SeniorCitizen <- as.factor(ifelse(dataset$SeniorCitizen==1, 'YES', 'NO'))
```

## Probemos un AutoML de H20 sin ninguna modificación

### Primero partimos los datos en split y test
```{r}
library(h2o)
h2o.init()

#Split data into Train/Validation/Test Sets

#split_h2o <- h2o.splitFrame(dataset.hex, c(0.6, 0.2), seed = 1234 )
#train_conv_h2o <- h2o.assign(split_h2o[[1]], "train" ) # 60%
#valid_conv_h2o <- h2o.assign(split_h2o[[2]], "valid" ) # 20%
#test_conv_h2o  <- h2o.assign(split_h2o[[3]], "test" )  # 20%

set.seed(37)
selected <- sample(1:nrow(dataset), 0.2*nrow(dataset))
train <- dataset[-selected,]
test <- dataset[selected,]
#Model
# Set names for h2o

y <- "Churn"
x <- setdiff(names(train), y)
```

```{r}
write.csv(train, file = "train.csv")
write.csv(test, file = "test.csv")

train2 = h2o.importFile("./train.csv")
test2 = h2o.importFile("./test.csv")



aml <- h2o.automl(x = x,
                  y = y,
                  training_frame = train2,
                  validation_frame = test2,
                  max_runtime_secs = 60,
                  exclude_algos = c("DeepLearning", "GLM", "DRF", "StackedEnsemble"))
```

Veamos cuál fue mejor:
```{r}

# Extract leader model
automl_leader <- aml@leader
automl_leader
```

```{r}
aml@leaderboard
```


```{r}
h2o.confusionMatrix(automl_leader)
```


Vale, vemos que el mejor modelo, nos dió una precisión del 84%. ¿Qué variables son las más importantes para que una persona haga churn o no?

```{r}
h2o.varimp_plot(automl_leader)

```

Las variables más importantes para predecir si una persona abandona o no, son:
* Contract: Duración de contrato.
* Tenure: Meses en la compañía (permanencia).
* Total charges: Cargos totales acumulados.



## EDA: Análisis exploratorio.

Para empezar, veamos qué porcentaje de la muestra tiene churn:
```{r}
brian_palette <- c("#6F78BD", "#EE8D47")

options(repr.plot.width = 6, repr.plot.height = 4)
dataset  %>% 
group_by(Churn) %>% 
summarise(Count = n())%>% 
mutate(percent = prop.table(Count)*100)%>%
ggplot(aes(reorder(Churn, -percent), percent), fill = Churn)+
geom_col(fill = brian_palette)+
geom_text(aes(label = sprintf("%.2f%%", percent)), hjust = 0.01,vjust = -0.5, size =3)+ 
theme_bw()+  
xlab("Churn") + 
ylab("Percent")+
ggtitle("Churn Percent")
```
Podría tratarse de una muestra no balanceada, lo cual es lógico en el ámbito de churn.


```{r}
options(repr.plot.width = 16, repr.plot.height = 70)

plot_grid(ggplot(dataset, aes(x=gender,fill=Churn))+ geom_bar()
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=SeniorCitizen,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=Partner,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=Dependents,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette)+theme_bw()
          +scale_x_discrete(labels = function(x)str_wrap(x, width = 1)),
          align = "v", ncol=2)
```

```{r}
plot_grid(ggplot(dataset, aes(x=PhoneService,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=MultipleLines,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=InternetService,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=OnlineSecurity,fill=Churn))+ geom_bar(position = 'fill')
                                +scale_fill_manual(values = brian_palette)
          +theme_bw()
          +scale_x_discrete(labels = function(x)str_wrap(x, width = 1)),
          align = "v", ncol=2)
```
```{r}
plot_grid(ggplot(dataset, aes(x=OnlineBackup,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=DeviceProtection,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=StreamingMovies,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=StreamingTV,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette)
          +theme_bw()
          +scale_x_discrete(labels = function(x)str_wrap(x, width = 1)),
          align = "v", ncol=2)

```
```{r}
plot_grid(ggplot(dataset, aes(x=Contract,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=PaperlessBilling,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette),
          ggplot(dataset, aes(x=PaymentMethod,fill=Churn))+ geom_bar(position = 'fill')
                      +scale_fill_manual(values = brian_palette)
          +theme_bw()
          +scale_x_discrete(labels = function(x)str_wrap(x, width = 1)),
          align = "v", ncol=2)
```


Podemos ver, por ejemplo:
* Las personas mayores son más propensas a abandonar la compañía.
* Las personas con dependientes, son menos propensas a abandonar la compañía.
* Las personas con múltiples líneas, tienen una ligera mayor proporción de abandonar.


```{r}
dataset %>%
  ggplot(aes(x=TotalCharges,fill=Churn))+   geom_density(alpha=0.8)+scale_fill_manual(values=brian_palette)+labs(title='Distribución de TotalCharges por churn' )
```

Podemos ver que a menores cargos acumulados, mayor proporción de abandonadores, y a medida que este valor crece, bajan los abandonadores. Se podría concluír que los clientes más fieles son los que están hace más años con la compañía.

Comprobémoslo:

```{r}
dataset %>%
  ggplot(aes(x=tenure,fill=Churn))+ geom_density(alpha=0.8)+scale_fill_manual(values=brian_palette)+labs(title='Distribución de TotalCharges por churn' )
```

Efectivamente, los clientes nuevos tienden a abandonar con más frecuencia que los que ya llevan un tiempo relativo en la empresa.

Hagamos un `boxplot` para ver las distribuciones de las variables continuas:

```{r}
library(GGally)

# Identificamos las variables continuas
class_vars <- sapply(dataset, class)
num_vars <- dataset[class_vars=="numeric"]
int_vars_names <- names(dataset)[class_vars=="integer"]

dataset[int_vars_names,] <- sapply(dataset[int_vars_names,], as.numeric)

num_vars <- dataset[class_vars=="numeric" | class_vars=="integer"]

ggpairs(num_vars, 
        title="Distribución de variables continuas",
        colours=brian_palette)

```

Podemos ver que existe cierta relación directa entre los cargos totales y el precio mensual.

```{r}
correlations <- cor(num_vars, use = "complete.obs")
correlations
ggcorrplot(correlations,
           type = "lower",
           title = "Correlación de variables numéricas",
           lab = TRUE,
           lab_size = 3,
           colors = c("white", "orange", "brown"))

```


```{r}
class_vars
```



## Preparación de datos

Veamos cuántos datos nulos tenemos:

```{r}
missing_data <- dataset %>% summarise_all(funs(sum(is.na(.))))
missing_data
```

¿cuáles son estos 11 casos que tienen `TotalCharges = NA`?
```{r}
nans = dataset[is.na(dataset$TotalCharges),]
nans
```

Suena lógico, tienen `TotalCharges` nulo, porque su `tenure` es 0, quiere decir que todavía no pagaron su primera cuota.

Esto lleva a preguntarme, ¿merece la pena tener estos casos para hacer una predicción? ¿Tienen información histórica suficiente como para aportar a nuestro modelo o más bien son outliers? ¿los rellenamos con 0 o los quitamos?

Veamos un boxplot de la columna `TotalCharges`:

```{r}
p <- ggplot(dataset, aes(x=Churn, y=tenure)) + 
  geom_boxplot()
p
```

Si bien el rango intercuartil de `tenure` para los abandonadores es más bajo, se decide quitar estos 11 casos, dado que no merecería la pena meter en el modelo a clientes que todavía no poseen información mínima histórica con la compañía.

```{r}
clean_dataset = dataset[!(is.na(dataset$TotalCharges)),]
clean_dataset
```

### Information value:


```{r}
#dataset$Churn <- as.numeric(ifelse(dataset$Churn=="YES", 1, 0))

clean_dataset$Churn <- as.numeric(ifelse(clean_dataset$Churn == "Yes", 1, 0))

iv_ds <- create_infotables(data=clean_dataset,
                           y="Churn")

iv_summary <- iv_ds$Summary
iv_summary <- iv_summary[order(iv_summary$IV), ]
iv_summary$Variable <- factor(iv_summary$Variable, levels=iv_summary$Variable)

ggplot(iv_summary, aes(x=Variable, y=IV, fill = IV))+
  coord_flip() +
  scale_fill_gradient(low = "grey", high = "green") +
  geom_bar(stat = "identity")

```
  

WOE segun la variable más importante `Contract`:


```{r}
ggplot(iv_ds$Tables$Contract, aes(x=Contract, y=WOE, fill = WOE))+
  scale_fill_gradient(low = "red", high = "green") +
  geom_bar(stat = "identity")
```

¿Y según `tenure`?:


```{r}
ggplot(iv_ds$Tables$tenure, aes(x=tenure, y=WOE, fill = WOE))+
  scale_fill_gradient(low = "red", high = "green") +
  geom_bar(stat = "identity")
```




## Preprocessing:

Hay variables categóricas que tienen el mismo valor, por lo tanto hay que modificarlas para notener misma información en dos variables si las pasamos a dummies:

```{r}
clean_dataset <- data.frame(lapply(clean_dataset, function(x) {
                  gsub("No internet service", "No", x)}))
clean_dataset <- data.frame(lapply(clean_dataset, function(x) {
                  gsub("No phone service", "No", x)}))
```

`tenure` es una variable continua, pero con valores absolutos. Para evitar problemas de ordinalidad, la clasificaremos en rangos:

```{r}
group_tenure <- function(tenure){
    if (tenure >= 0 & tenure <= 12){
        return('0-12 Month')
    }else if(tenure > 12 & tenure <= 24){
        return('12-24 Month')
    }else if (tenure > 24 & tenure <= 48){
        return('24-48 Month')
    }else if (tenure > 48 & tenure <=60){
        return('48-60 Month')
    }else if (tenure > 60){
        return('> 60 Month')
    }
}


clean_dataset$tenure <- as.numeric(clean_dataset$tenure)
clean_dataset$tenure_group <- sapply(clean_dataset$tenure,group_tenure)
clean_dataset$tenure_group <- as.factor(clean_dataset$tenure_group)

clean_dataset$tenure <- NULL
```




```{r}
df <- clean_dataset[,names(clean_dataset) != "customerID"]



df$MonthlyCharges <- as.numeric(df$MonthlyCharges)
df$TotalCharges <- as.numeric(df$TotalCharges)

class_vars <- sapply(df, class)

cat_vars <- df[,class_vars=="factor"]
cat_vars$Churn <- NULL

class_vars

```



```{r}
library("fastDummies")

Dummies <- dummy_cols(df, select_columns = names(cat_vars), remove_first_dummy = TRUE)

df <- Dummies[,!names(Dummies) %in% names(cat_vars)]

df$Churn <- as.factor(df$Churn)
```


## Modelos de clasificación:

Primero hacemos un split de las variables:


```{r}
library("caret")

seed <- 12345 
  
set.seed(seed)

trainIndex <- createDataPartition(df$Churn, p = .80, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)
```

Separamos el dataframe en muestras de entrenamiento y de evaluación de resultados:

```{r}
Train <- df[ trainIndex,]
Test  <- df[-trainIndex,]

```

```{r}
head(Train)
```



Debug de conexión de Rstudio a las paralelizaciones de los modelos:
```{r}
# Installation of the doSNOW parallel library with all dependencies
doInstall <- TRUE # Change to FALSE if you don't want packages installed.
toInstall <- c("doSNOW") 
if((doInstall) && (!is.element(toInstall, installed.packages()[,1])))
{
    cat("Please install required package. Select server:"); chooseCRANmirror();
    install.packages(toInstall, dependencies = c("Depends", "Imports")) 
}
# load doSnow and (parallel for CPU info) library
library(doSNOW)
library(parallel)
# For doSNOW one can increase up to 128 nodes
# Each node requires 44 Mbyte RAM under WINDOWS.
# detect cores with parallel() package
nCores <- detectCores(logical = FALSE)
cat(nCores, " cores detected.")
# detect threads with parallel()
nThreads<- detectCores(logical = TRUE)
cat(nThreads, " threads detected.")
# Create doSNOW compute cluster (try 64)
# One can increase up to 128 nodes
# Each node requires 44 Mbyte RAM under WINDOWS.
cluster = makeCluster(nThreads, type = "SOCK")
class(cluster);
# register the cluster
registerDoSNOW(cluster)
#get info
getDoParWorkers(); getDoParName();
# insert parallel computation here
        
# stop cluster and remove clients
stopCluster(cluster); print("Cluster stopped.")
# insert serial backend, otherwise error in repetetive tasks
registerDoSEQ()
# clean up a bit.
invisible(gc); remove(nCores); remove(nThreads); remove(cluster); 
```

### Regresión logística

```{r}

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10,
                           )


## Logistic regression
lreg<-train(Churn ~ ., data = Train,
            method="glm",
            family=binomial(),
            trControl=fitControl)
summary(lreg)

```

Vamos a sacar variables que tienen el p-value muy alto o mucho rango de error:

```{r}
# Creamos el nuevo Train y test:
Train2 <-  Train
Train2$MultipleLines_Yes <- NULL
Train2$Dependents_Yes <- NULL

Test2 <- Test
Test2$MultipleLines_Yes <- NULL
Test2$Dependents_Yes <- NULL


#Probamos nuevo modelo:

lreg2 <-train(Churn ~ ., data = Train2,
            method="glm",
            family=binomial(),
            trControl=fitControl)

lreg2
summary(lreg2)


```

Veamos la matriz de confusión para este modelo:
```{r}

model_glm_predict <- predict(lreg2,Test2)
confusionMatrix(model_glm_predict,Test2$Churn, positive = "1")

```

No resultó siendo un modelo muy bueno, ya que podemos ver que existe una gran cantidad de falsos positivos y hay bastantes abandonadores que no los reconoce.



### Adaboost


```{r}
library(fastAdaboost)


ada = adaboost(Churn~.,
               Train, 
               nIter = 40
               )

ada
summary(ada)
```

Veamos los resultados:
```{r}
model_ada_predict <- predict(ada,newdata =  Test)

confusionMatrix(model_ada_predict$class, Test$Churn, positive = "1")


```

Con este segundo modelo, obtenemos un peor resultado, en cuanto a accuracy y falsos negativos.


### Random Forest

No soportado por mi portátil.

### XGBoost

```{r}
grid_default <- expand.grid(
  nrounds = 100,
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)


train_control <- caret::trainControl(
  method = "none",
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_base <- caret::train(
  Churn~.,
  Train, 
  trControl = train_control,
  tuneGrid = grid_default,
  method = "xgbTree",
  verbose = TRUE
)

xgb_base


```

```{r}
xgb_base_pred <- predict(xgb_base, Test)
confusionMatrix(xgb_base_pred, Test$Churn, positive = "1")
```

Vamos a hacer un gridsearch para los hiperparámetros:

```{r}
nrounds <- 500

tune_grid <- expand.grid(
  nrounds = seq(from = 200, to = nrounds, by = 50),
  eta = c(0.025, 0.05, 0.1, 0.3),
  max_depth = c(2, 3, 4, 5, 6),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

tune_control <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3, # with n folds 
  #index = createFolds(tr_treated$Id_clean), # fix the folds
  verboseIter = FALSE, # no training log
  allowParallel = TRUE # FALSE for reproducible results 
)

xgb_tune <- caret::train(
  Churn~.,
  Train, 
  trControl = tune_control,
  tuneGrid = tune_grid,
  method = "xgbTree",
  verbose = TRUE
)

xgb_tune$bestTune
```

Veamos los resultados del grid resultado aplicando también un resampling de tipo SMOTE:
```{r}
grid_new <- expand.grid(
  nrounds = 250,
  max_depth = 4,
  eta = 0.025,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)


train_control <- caret::trainControl(
  method = "none",
  verboseIter = FALSE, # no training log
  allowParallel = TRUE, # FALSE for reproducible results 
  sampling = "smote"
  )

xgb_tuned <- caret::train(
  Churn~.,
  Train, 
  trControl = train_control,
  tuneGrid = grid_new,
  method = "xgbTree",
  verbose = TRUE
)

xgb_tuned
```
```{r}

model_xgb_predict <- predict(xgb_tuned,newdata =  Test)

confusionMatrix(model_xgb_predict, Test$Chur, positive = "1")
```

Es mejor que el *Adaboost* pero peor que el *GLM*


Por otro lado, hemos visto en el H2o que el modelo que mejor performaba era el *GBM*, ¿y si lo probamos?

### GBM

```{r}
gbmGrid <-  grid <- expand.grid( .n.trees=seq(10,50,10), 
                                 .interaction.depth=seq(1,4,1), 
                                 .shrinkage=c(0.01,0.001), 
                                 .n.minobsinnode=seq(5,20,5)) 

fitControl <- trainControl(method = "repeatedcv",
                       repeats = 5,
                       preProcOptions = list(thresh = 0.95),
                       ## Estimate class probabilities
                       classProbs = TRUE,
                       ## Evaluate performance using
                       ## the following function
                       summaryFunction = twoClassSummary)

# Method + Date + distribution
set.seed(12345)

Train3 <- Train

Train3 %>% 
  rename(
    PaymentMethod_Bank_transfer = "PaymentMethod_Bank transfer (automatic)",
    PaymentMethod_CreditCard = "PaymentMethod_Credit card (automatic)",
    InternetService_FiberOptic = "InternetService_Fiber optic",
    Contract_OneYear = "Contract_One year",
    Contract_TwoYear = "Contract_Two year",
    PaymentMethod_MailedCheck = "PaymentMethod_Mailed check",
    tenure_group_more_60Month = "tenure_group_> 60 Month",
    tenure_group_24_48Month = "tenure_group_24-48 Month",
    tenure_group_12_24Month = "tenure_group_12-24 Month",
    tenure_group_48_60Month = "tenure_group_48-60 Month"

    )

feature.names=names(Train3)

for (f in feature.names) {
  if (class(Train3[[f]])=="factor") {
    levels <- unique(c(Train3[[f]]))
    Train3[[f]] <- factor(Train3[[f]],
                   labels=make.names(levels))
  }
}

GBMada <- train(Churn~.,
            Train3, 
            distribution = "adaboost",
            method = "gbm",
            trControl = fitControl,
            verbose = TRUE,
            tuneGrid = gbmGrid,
            ## Specify which metric to optimize
            metric = "Accuracy")

GBMada
```

```{r}
whichTwoPct <- tolerance(GBMada$results, metric = "ROC", tol = 2, maximize = TRUE)
cat("best model within 2 pct of best:\n")
GBMada$results[whichTwoPct,1:6]
```

Es bastante mejor el resultado, intentemos tirar directamente ese modelo:
```{r}
GBMada$bestTune
```
```{r}
gbmGrid_best <-  grid <- expand.grid( 
                                 .n.trees=50, 
                                 .interaction.depth=4, 
                                 .shrinkage=0.01, 
                                 .n.minobsinnode=20) 

fitControl <- trainControl(method = "repeatedcv",
                       repeats = 5,
                       preProcOptions = list(thresh = 0.95),
                       ## Estimate class probabilities
                       classProbs = TRUE,
                       ## Evaluate performance using
                       ## the following function
                       summaryFunction = twoClassSummary)

GBMada_best <- train(Churn~.,
            Train3, 
            distribution = "adaboost",
            method = "gbm",
            trControl = fitControl,
            verbose = TRUE,
            tuneGrid = gbmGrid_best,
            ## Specify which metric to optimize
            metric = "ROC",)

GBMada_best
```



```{r}
Test3 <- Test

Test3 %>% 
  rename(
    PaymentMethod_Bank_transfer = "PaymentMethod_Bank transfer (automatic)",
    PaymentMethod_CreditCard = "PaymentMethod_Credit card (automatic)",
    InternetService_FiberOptic = "InternetService_Fiber optic",
    Contract_OneYear = "Contract_One year",
    Contract_TwoYear = "Contract_Two year",
    PaymentMethod_MailedCheck = "PaymentMethod_Mailed check",
    tenure_group_more_60Month = "tenure_group_> 60 Month",
    tenure_group_24_48Month = "tenure_group_24-48 Month",
    tenure_group_12_24Month = "tenure_group_12-24 Month",
    tenure_group_48_60Month = "tenure_group_48-60 Month"

    )


feature.names=names(Test3)

for (f in feature.names) {
  if (class(Test3[[f]])=="factor") {
    levels <- unique(c(Test3[[f]]))
    Test3[[f]] <- factor(Test3[[f]],
                   labels=make.names(levels))
  }
}

mPred = predict(GBMada_best, Test3, na.action = na.pass)
confusionMatrix(mPred, Test3$Churn, positive = "X1")

```

Sólo encuentra un valor en los Test, por lo que el modelo no nos sirve.



### Árbol de decisión

```{r}
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)

set.seed(12345)

colnames(Train) <- make.names(colnames(Train))

dtree_fit <- train(Churn~.,
                   Train,
                   method = "rpart",
                   parms = list(split = "information"),
                   trControl=trctrl,
                   tuneLength = 10,
                   )

dtree_fit
```

```{r}

colnames(Test) <- make.names(colnames(Test))

predict_tree <-  predict(dtree_fit, Test)
confusionMatrix(predict_tree, Test$Churn, positive = "1")
```

Nuevamente tenemos un problema con los positivos que no detecta, ¿y si hacemos un resampling?

### Tuning GLM
```{r}
modelLookup("bayesglm")
```




```{r}

fitControl <- trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10,
                           sampling = "smote"
                           )

set.seed(12345)

lreg2 <-train(Churn ~ .,
              data = Train2,
              method="glm",
              family=binomial(),
              trControl=fitControl,
              preProc = c("center", "scale"))

lreg2
```

```{r}

predict_glm <-  predict(lreg2, Test2)
confusionMatrix(predict_glm, Test2$Churn, positive = "1")
```
```{r}
varImp(lreg2, scale = FALSE)
```

Esta sería nuestra mejor aproximación, donde vemos reducida la cantidad de falsos negativos.

## Conclusiones:

Al tratarse de una muestra no balanceada, se tuvo que recurrir a técnicas de balanceo como el "SMOTE".

Las variables que más peso tienen en la predicción analizada son:
  * Contrato (2 años y 1 año).
  * Servicio de internet (fibra).
  * Tenure 48-60 meses (mucho tiempo en compañía).


Se recomienda a la empresa cerrar contratos largos, ofrecer fibra óptica en lugar de adsl y hacer una acción especial regalando fibra y generando contratos a largo plazo.

